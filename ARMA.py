import tensorflow as tf
from tensorflow.keras import backend as K
import spektral
import numpy as np
import math
from sklearn.feature_selection import mutual_info_regression

class Weight(tf.keras.callbacks.Callback):    # DWA
    def __init__(self, w0, w1, w2, w3): # å¿…é¡»åˆ†å¼€ï¼Œä¸èƒ½æ•´åˆæˆä¸€ä¸ªå¼ é‡ğŸ˜”
        self.w0, self.w1, self.w2, self.w3 = w0, w1, w2, w3
        self.K = 4
        self.Loss = [list() for i in range(self.K)]
        super(Weight, self).__init__()

    def on_epoch_end(self, epoch, logs=None):
        self.Loss[0].append(logs.get("label_loss"))
        self.Loss[1].append(logs.get("lc_loss"))
        self.Loss[2].append(logs.get("coral_loss"))
        self.Loss[3].append(logs.get("mi_loss"))

    def on_epoch_begin(self, epoch, logs=None):
        T = 1.
        if epoch >= 2:
            wKt = list()
            for k in range(self.K):
                Lkt1, Lkt2 = self.Loss[k][epoch-1], self.Loss[k][epoch-2]
                wKt.append(Lkt1 / Lkt2)

            lambdaKt = list()
            wKt = [wkt/T - max(wKt)/T for wkt in wKt]
            for wkt in wKt:
                lambdaKt.append(self.K * math.exp(wkt) / sum(math.exp(wkt) for wkt in wKt))

            K.set_value(self.w0, lambdaKt[0])
            K.set_value(self.w1, lambdaKt[1])
            K.set_value(self.w2, lambdaKt[2])
            K.set_value(self.w3, lambdaKt[3])
        print(f'\nç¬¬{epoch+1}ä¸ªepochçš„Weightä¸º{K.get_value(self.w0):.3f} {K.get_value(self.w1):.3f} {K.get_value(self.w2):.3f} {K.get_value(self.w3):.3f}')

def squareplus(x):
    b = 4
    return 0.5* (tf.math.sqrt(tf.math.pow(x,2) +b) +x)

class Encoder(tf.keras.layers.Layer):
    def __init__(self, AdjMat):
        self.AdjMat = AdjMat # (channels, channels)
        super(Encoder, self).__init__(trainable=True)

    def build(self, input_shape):
        features = input_shape[2]
        self.ARMA = spektral.layers.ARMAConv(features, order=3, iterations=7, share_weights=True,
                                             gcn_activation=squareplus, dropout_rate=0.48, activation=squareplus,
                                             use_bias=True, kernel_initializer='he_uniform')
        super(Encoder, self).build(input_shape)

    def call(self, eeg):    # [(batch, channels, features)]
        return self.ARMA([eeg, self.AdjMat])    # ([batch, channels, features])

class Decoder(tf.keras.layers.Layer):   # å‚è€ƒhttps://github.com/danielegrattarola/spektral/blob/master/spektral/layers/convolutional/gat_conv.py
    def __init__(self, AdjMat):
        self.AdjMat = AdjMat # ç”¨ï¼ˆæ­£åˆ™åŒ–ï¼‰é‚»æ¥çŸ©é˜µç»™attn_coefæ‰“mask
        super(Decoder, self).__init__(trainable=True)

    def build(self, input_shape):   # ([batch, channels, features])
        # batch = input_shape[0]  # ä¸æŒ‡å®šbatch_sizeåˆ™ä¸èƒ½ç´¢å¼•input_shape[0]ï¼ï¼ï¼
        # channels = input_shape[1]
        features = input_shape[2] # è¾“å…¥è¾“å‡ºèŠ‚ç‚¹ç‰¹å¾ç»´åº¦ä¸å˜

        attn_heads = 1
        self.kernel = self.add_weight(name="kernel", shape=[features, attn_heads, features])
        self.attn_kernel_self = self.add_weight(name="attn_kernel_self", shape=[features, attn_heads, 1])
        self.attn_kernel_neighs = self.add_weight(name="attn_kernel_neighs", shape=[features, attn_heads, 1])

        # self.diag0 = tf.zeros([batch, channels])
        super(Decoder, self).build(input_shape)

    def call(self, encode): # einsumå¤ªç¥å¥‡äº†
        encode = tf.einsum("...NI , IHO -> ...NHO", encode, self.kernel)
        attn_for_self = tf.einsum("...NHI , IHO -> ...NHO", encode, self.attn_kernel_self)
        attn_for_neighs = tf.einsum("...NHI , IHO -> ...NHO", encode, self.attn_kernel_neighs)
        attn_for_neighs = tf.einsum("...ABC -> ...CBA", attn_for_neighs)    # æ²¡çœ‹æ‡‚ğŸ˜…
        attn_coef = attn_for_self + attn_for_neighs # ä¸åŸæ–‡ç­‰ä»·ï¼Œè¯¦è§https://github.com/dmlc/dgl/blob/c4aa74baf80551515c8f58244137deedea920172/python/dgl/nn/tensorflow/conv/gatconv.py
        attn_coef = tf.math.reduce_mean(attn_coef, axis=2, keepdims=False)  # æ³¨æ„åŠ›å¤´å¹³å‡ï¼›([batch, channels, channels])

        attn_coef = tf.math.tanh(attn_coef /48.)    # ç¼©å°48å€ğŸ˜œï¼
        attn_coef = tf.where(attn_coef<0., 0., attn_coef)
        # attn_coef = tf.nn.relu(attn_coef)   # ç»éªŒè¯ï¼Œä¸ä¸Šå¼ç­‰ä»·

        mask = tf.where(self.AdjMat==0, 0., 1.)
        mask = tf.tile(tf.expand_dims(mask,0), (tf.shape(encode)[0],1,1))
        attn_coef = tf.math.multiply_no_nan(attn_coef, mask)
        return attn_coef # ([batch, channels, channels])
# -----------------------------------------------------------------------------
# class AdjMat2RL(tf.keras.layers.Layer):
#     def __init__(self):
#         super(AdjMat2RL, self).__init__(trainable=False)

#     def build(self, input_shape):   # ([batch, channels, channels])
#         # self.batch = input_shape[0]
#         # self.channels = input_shape[1]

#         super(AdjMat2RL, self).build(input_shape)

#     def call(self, AdjMat):
#         # RL = tf.zeros([self.batch, self.channels, self.channels])
#         RL = tf.zeros(tf.shape(AdjMat))
#         # for i in tf.range(self.batch):
#         for i in tf.range(tf.shape(AdjMat)[0]):
#             AdjMat_i = AdjMat[i,:,:]

#             # NL_i = tf.numpy_function(spektral.utils.normalized_laplacian, [AdjMat_i, True], tf.float32)    # symmetric: boolean, compute symmetric normalization
#             # RL_i = tf.numpy_function(spektral.utils.rescale_laplacian, [NL_i], tf.float32)
#             RL_i = tf.numpy_function(spektral.utils.normalized_adjacency, [AdjMat_i, True], tf.float32) # å®é™…åº”è¯¥ç”¨å¯¹ç§°ï¼ˆä¸å¯¹ç§°å¯ä»¥å—ï¼Ÿï¼‰æ­£åˆ™åŒ–é‚»æ¥çŸ©é˜µï¼Œè€Œä¸æ˜¯ä¸Šé¢ä¸¤è¡Œæ‹‰æ™®æ‹‰æ–¯ï¼Ÿ

#             RL_i = tf.expand_dims(RL_i, axis=0)
#             RL = tf.tensor_scatter_nd_update(RL, [[i]], RL_i)   # å­¦ä¼šè¿™ç§æ›´æ–°å¼ é‡çš„æ–¹å¼
#         # ................................................
#         # NL = tf.map_fn(lambda Mat: tf.numpy_function(spektral.utils.normalized_laplacian, [Mat], tf.float16), elems=AdjMat, fn_output_signature=tf.float16) # ä¸ºä½•æœ‰é”™ï¼Ÿ
#         # RL = tf.map_fn(lambda Mat: tf.numpy_function(spektral.utils.rescale_laplacian, [Mat], tf.float16), elems=NL, fn_output_signature=tf.float16)
#         return RL # ([batch, channels, channels])

def compute_coral(Ds, Dt):
    ns, nt = tf.cast(tf.shape(Ds)[0], 'float32'), tf.cast(tf.shape(Dt)[0], 'float32')
    d = tf.cast(tf.shape(Ds)[1], 'float32') # æˆ–tf.cast(tf.shape(Dt)[1], 'float32')

    DsTDs = tf.linalg.matmul(Ds, Ds, transpose_a=True)
    onesTDs = tf.linalg.matmul(tf.ones([ns,1]), Ds, transpose_a=True)   # tf.onesæ”¯æŒfloatç»´åº¦
    Cs = 1./(ns-1.) * \
        (DsTDs - 1./ns*tf.linalg.matmul(onesTDs, onesTDs, transpose_a=True)) # ([features, features])

    DtTDt = tf.linalg.matmul(Dt, Dt, transpose_a=True)
    onesTDt = tf.linalg.matmul(tf.ones([nt,1]), Dt, transpose_a=True)
    Ct = 1./(nt-1.) * \
        (DtTDt - 1./nt*tf.linalg.matmul(onesTDt, onesTDt, transpose_a=True))

    coral = 1. / (4.*tf.math.square(d)) * \
        tf.math.square(tf.norm(Cs-Ct, ord='euclidean'))
    return coral

def compute_coral_c(Ds, Dt, ls, lt, c):   # D: float32(batch, d); l: int32 (batch); c: int32!!!
    coral_c = tf.TensorArray(dtype='float32', size=c, clear_after_read=False)
    for i in tf.range(c):
        Ds_i = tf.gather(Ds, indices=tf.squeeze(tf.where(ls==i)))   # tf.whereè¿”å›å¤šä¸€ä¸ªæ— ç”¨ç»´åº¦
        Dt_i = tf.gather(Dt, indices=tf.squeeze(tf.where(lt==i)))

        ns_i = tf.size(tf.where(ls==i), out_type=tf.dtypes.float32)
        nt_i = tf.size(tf.where(lt==i), out_type=tf.dtypes.float32)
        if ns_i < 2 or nt_i < 2:
            # raise ValueError('æŸä¸ªåŸŸçš„æŸä¸ªç±»åˆ«æ ·æœ¬æ•°å¤ªå°‘')   # åªæœ‰åˆ†ç±»åç”±äºbatchä¸­æ ·æœ¬çš„éšæœºæ€§å¯èƒ½å¯¼è‡´æŸç±»æ ·æœ¬ç¼ºå¤±
            coral_i = 0.
        else:
            coral_i = compute_coral(Ds_i, Dt_i)

        coral_c = coral_c.write(i, coral_i)

    coral_c = coral_c.stack()
    coral_c = tf.math.reduce_mean(coral_c)  # å„ç±»æ±‚å¹³å‡ï¼ğŸ˜Š
    return coral_c

class CORAL_C(tf.keras.losses.Loss):    # æŒ‰ç±»åˆ«ç›¸äº’å¯¹é½ï¼ˆä¸åŒç±»åˆ†åˆ«å¯¹é½åæ±‚å¹³å‡ï¼‰
    def __init__(self):
        super().__init__(reduction=tf.keras.losses.Reduction.NONE, name='Correlation_alignment_class')   # æŸå¤±å‡½æ•°è¿”å›é•¿åº¦ä¸ºbatch sizeçš„å‘é‡ï¼Œmodel.fitæ—¶è‡ªåŠ¨æ±‚å¹³å‡ï¼›æˆ–è€…æå‰è‡ªå·±æ±‚å¹³å‡ï¼›æ³¨æ„è¿™é‡Œå¦‚ä½•å‘çˆ¶ç±»ä¼ é€’å‚æ•°

    def call(self, y_true, y_pred): # int8([batch, domains])ï¼›([batch, domains, features])
        domains = tf.shape(y_true)[1]
        c = tf.constant(2) # å‡å®šå°±ä¸¤ç±»ï¼

        Coral_r = tf.TensorArray(dtype='float32', size=domains, clear_after_read=False) # sizeå¿…ä¸ºint32
        for i in tf.range(domains):
            Coral_c = tf.TensorArray(dtype='float32', size=domains, clear_after_read=False)
            for j in tf.range(domains):
                if j > i:   # æ¯”è¿‡çš„ä¸å†æ¯”ï¼Œä¸”ä¸è·Ÿè‡ªå·±æ¯”
                    Ds = y_pred[:,i,:]  # ([batch, features])
                    Dt = y_pred[:,j,:]

                    ls = tf.cast(y_true[:,i], 'int32')  # å¯ä»¥è‡ªåŠ¨ç²¾åº¦è½¬æ¢å—ï¼Ÿ
                    lt = tf.cast(y_true[:,j], 'int32')

                    coral = compute_coral_c(Ds, Dt, ls, lt, c)
                else:
                    coral = tf.constant(0.)

                Coral_c = Coral_c.write(j, coral)

            Coral_c = Coral_c.stack()
            Coral_r = Coral_r.write(i, Coral_c)

        Coral_r = Coral_r.stack()   # å³ä¸Šä¸‰è§’æ— ä¸»å¯¹è§’çº¿([domains, domains])

        domains = tf.cast(domains, 'float32')
        Coral = tf.math.reduce_sum(Coral_r) / (domains*(domains-1.)/2.)   # è¢«è¯•ï¼ˆåŸŸï¼‰å¯¹å„¿å¹³å‡ï¼ğŸ˜Š

        return Coral
# -------------------------------------- è€CORALï¼ˆç›¸äº’å¯¹é½ä¸ç®¡ç±»åˆ«ï¼‰ ---------------------------------------
# class CORAL(tf.keras.losses.Loss):  # ç»éªŒè¯ï¼Œä¸ä»–äººåŸºæœ¬ç›¸ç­‰
#     def __init__(self):
#         super().__init__(reduction=tf.keras.losses.Reduction.NONE, name='Correlation_alignment')   # æŸå¤±å‡½æ•°è¿”å›é•¿åº¦ä¸ºbatch sizeçš„å‘é‡ï¼Œmodel.fitæ—¶è‡ªåŠ¨æ±‚å¹³å‡ï¼›æˆ–è€…æå‰è‡ªå·±æ±‚å¹³å‡ï¼›æ³¨æ„è¿™é‡Œå¦‚ä½•å‘çˆ¶ç±»ä¼ é€’å‚æ•°

#     def call(self, y_true, y_pred): # y_trueä¸ç”¨ï¼Œéšä¾¿èµ‹ä¸€ä¸ªï¼›([batch, domains, features])
#         batch = tf.cast(tf.shape(y_pred)[0], 'float32')
#         domains = tf.shape(y_pred)[1]   # è‡ªåŠ¨tf.int32
#         features = tf.cast(tf.shape(y_pred)[2], 'float32')

#         if domains > 1: # è‡³å°‘æœ‰ä¸¤ä¸ªåŸŸï¼›tfå¸ƒå°”å¯ä»¥è¿™æ ·
#             Coral_r = tf.TensorArray(dtype='float32', size=domains, dynamic_size=False, clear_after_read=False) # sizeå¿…ä¸ºint32
#             for i in tf.range(domains):
#                 Coral_c = tf.TensorArray(dtype='float32', size=domains, dynamic_size=False, clear_after_read=False)
#                 for j in tf.range(domains):
#                     if j > i:   # æ¯”è¿‡çš„ä¸å†æ¯”ï¼Œä¸”ä¸è·Ÿè‡ªå·±æ¯”
#                         Ds = y_pred[:,i,:]  # ([batch, features])
#                         Dt = y_pred[:,j,:]

#                         DsTDs = tf.linalg.matmul(Ds, Ds, transpose_a=True)
#                         onesTDs = tf.linalg.matmul(tf.ones([batch,1]), Ds, transpose_a=True)
#                         Cs = 1./(batch-1.) * \
#                             (DsTDs - 1./batch*tf.linalg.matmul(onesTDs, onesTDs, transpose_a=True)) # ([features, features])

#                         DtTDt = tf.linalg.matmul(Dt, Dt, transpose_a=True)
#                         onesTDt = tf.linalg.matmul(tf.ones([batch,1]), Dt, transpose_a=True)
#                         Ct = 1./(batch-1.) * \
#                             (DtTDt - 1./batch*tf.linalg.matmul(onesTDt, onesTDt, transpose_a=True))

#                         coral = 1./(4.*tf.math.square(features)) * \
#                                 tf.math.square(tf.norm(Cs-Ct, ord='euclidean'))   # Default is 'euclidean' which is equivalent to Frobenius norm if tensor is a matrix and equivalent to 2-norm for vectorsï¼›ç­‰ä»·äºtf.reduce_sum(tf.multiply(Cs-Ct, Cs-Ct))
#                     else:
#                         coral = tf.constant(0.)

#                     Coral_c = Coral_c.write(j, coral)

#                 Coral_c = Coral_c.stack()
#                 Coral_r = Coral_r.write(i, Coral_c)

#             Coral_r = Coral_r.stack()   # å³ä¸Šä¸‰è§’æ— ä¸»å¯¹è§’çº¿([domains, domains])

#             domains = tf.cast(domains, 'float32')
#             Coral = tf.math.reduce_sum(Coral_r) / (domains*(domains-1.)/2.)   # å¹³å‡ï¼›([])

#         else:
#             Coral = tf.constant(0.) # ([])

#         return Coral

def mutual_information(x, y):
    X = np.expand_dims(x, axis=-1)
    mi = mutual_info_regression(X, y, discrete_features=False, n_neighbors=3)
    return mi.astype('float32') # é»˜è®¤float64

class Mutual_Information(tf.keras.losses.Loss):
    def __init__(self):    # ç»éªŒè¯ï¼ŒSUM_OVER_BATCH_SIZEå°†ä¸€ä¸ªbatchå–å‡å€¼ï¼
        super().__init__(reduction=tf.keras.losses.Reduction.NONE, name='Mutual_information')

    def call(self, y_true, y_pred): # ([batch, domains, 2, features])
        batch = tf.shape(y_pred)[0]
        domains = tf.shape(y_pred)[1]
        MI = tf.TensorArray(dtype='float32', size=batch, dynamic_size=False, clear_after_read=False)    # sizeå¿…ä¸ºint32ï¼

        for i in tf.range(batch):
            mi = tf.TensorArray(dtype='float32', size=domains, dynamic_size=False, clear_after_read=False)

            for j in tf.range(domains):
                encode_public = y_pred[i, j, 0, :]
                encode_private = y_pred[i, j, 1, :]

                mutual_info = tf.numpy_function(mutual_information, [encode_public, encode_private], tf.float32)   # self.è¡¨ç¤ºç±»å†…è°ƒç”¨
                mutual_info = tf.squeeze(mutual_info)   # ([])

                mi = mi.write(j, mutual_info)

            mi = mi.stack()
            MI = MI.write(i, mi)

        MI = MI.stack()  # # ([batch, domains])
        MI = tf.math.reduce_mean(MI, axis=(-1,-2))  # å¹³å‡batchå’Œè¢«è¯•ï¼ˆåŸŸï¼‰ï¼ğŸ˜Š

        return MI # ç»´åº¦å’Œç²¾åº¦å‡ä¸å˜

def percent(A):
    channels = 64
    ratio = .2
    A_sort = np.sort(np.ravel(A))[::-1]
    a = int(np.floor(channels * channels * ratio))  # ????????????????????????????????????????????????????????????????????????????
    A[A < A_sort[a]] = 0.
    return A

def AAD(sources, channels, features, domains, AdjMat):  # AdjMatä¸ä½œä¸ºmodelçš„Input
    # ------------------------------ è¾“å…¥ ------------------------------
    eeg_in = tf.keras.Input(shape=(domains, channels, features))   # ([batch, domains, channels, features])
    eeg = tf.unstack(eeg_in, num=domains, axis=1)  # è¿”å›åˆ—è¡¨[domains]ï¼Œæ¯ä¸ªå…ƒç´ ä¸º([batch, channels, features])

    LN = tf.keras.layers.LayerNormalization(axis=(1,2)) # å…±äº«ğŸ‘
    eeg = [LN(eeg[i]) for i in range(domains)]

    AdjMat_public = tf.math.reduce_mean(AdjMat, axis=0) # å…±äº«ğŸ‘
    AdjMat_private = tf.unstack(AdjMat, num=domains, axis=0)    # (domainsä¸ª[channels, channels])

    AdjMat_public = tf.numpy_function(percent, [AdjMat_public], tf.float32)
    AdjMat_private = [tf.numpy_function(percent, [AdjMat_private[i]], tf.float32) for i in range(domains)]

    AdjMat_public = tf.numpy_function(spektral.utils.normalized_adjacency, [AdjMat_public, True], tf.float32)
    AdjMat_private = [tf.numpy_function(spektral.utils.normalized_adjacency, [AdjMat_private[i], True], tf.float32) for i in range(domains)]
    # ----------------------------- â–  ç‰¹å¾è§£è€¦ ------------------------------
    Encoder_public = Encoder(AdjMat_public) # å…±äº«ğŸ‘
    encode_public = [Encoder_public(eeg[i]) for i in range(domains)]

    Encoder_private = [Encoder(AdjMat_private[i]) for i in range(domains)]
    encode_private = [Encoder_private[i](eeg[i]) for i in range(domains)]

    Add = tf.keras.layers.Add()
    encode = [Add([encode_public[i], encode_private[i]]) for i in range(domains)]   # ([batch, channels, features])
    # ................................................
    Decoder_all = [Decoder(AdjMat_private[i]) for i in range(domains)]
    AdjMat_rec = [Decoder_all[i](encode[i]) for i in range(domains)]   # é‡æ„ç§æœ‰é‚»æ¥çŸ©é˜µï¼›([batch, channels, channels])
    # ------------------------------------------------------------------
    Pooling = spektral.layers.GlobalAvgPool()   # å„é€šé“å¹³å‡

    encode_public = [Pooling(encode_public[i]) for i in range(domains)]  # ([batch, features])
    encode_private = [Pooling(encode_private[i]) for i in range(domains)]
    # ------------------------ â˜… åŸŸæ³›åŒ–ï¼šCORAL ------------------------
    Stack_public = tf.keras.layers.Lambda(lambda encode_public: tf.stack([encode_public[i] for i in range(domains)], axis=1), name='coral')
    Stack_private = tf.keras.layers.Lambda(lambda encode_private: tf.stack([encode_private[i] for i in range(domains)], axis=1))
    Stack_AdjMat = tf.keras.layers.Lambda(lambda AdjMat_rec: tf.stack([AdjMat_rec[i] for i in range(domains)], axis=1), name='lc')

    encode_public = Stack_public(encode_public) # ([batch, domains, features])
    encode_private = Stack_private(encode_private)
    AdjMat_rec = Stack_AdjMat(AdjMat_rec)

    encode_public_private = tf.keras.layers.concatenate(
        [tf.expand_dims(encode_public, 2), tf.expand_dims(encode_private, 2)],
        axis=2, name='mi')  # ([batch, domains, 2, features])
    # -------------------------- â–  FC ---------------------------
    Dense = tf.keras.layers.Dense(round((features+sources)/2), activation=squareplus, kernel_initializer='he_uniform') # å…±äº«æ ‡ç­¾åˆ†ç±»å™¨
    BN = tf.keras.layers.BatchNormalization()
    Softmax = tf.keras.layers.Dense(sources, activation='softmax')    # ç»™è¾“å‡ºDenseå±‚å‘½å

    label = tf.keras.models.Sequential([Dense, BN, Softmax], name='label')(encode_public)    # ([batch, domain, sources])
    # ---------------------------- æ„å»ºæ¨¡å‹ -----------------------------
    model = tf.keras.Model(inputs=eeg_in,
                           outputs=[label, AdjMat_rec, encode_public, encode_public_private])
    model.summary(show_trainable=False)
    global w0, w1, w2, w3
    w0, w1, w2, w3 = K.variable(1.), K.variable(0.), K.variable(0.), K.variable(0.)
    model.compile(loss=['sparse_categorical_crossentropy', 'log_cosh', CORAL(), Mutual_Information()],   # æ‰€æœ‰losséƒ½æ˜¯batchå’Œè¢«è¯•ï¼ˆåŸŸï¼‰ä¸Šå¹³å‡ï¼ğŸ˜Šlossç²¾åº¦è‡ªåŠ¨è·Ÿéšè¾“å…¥ï¼ˆint8ä¸float16è¾“å‡ºä¸ºfloat16ï¼‰
                  loss_weights=[w0, w1, w2, w3],    # åœ¨è‡ªå®šä¹‰Callbackä¸­è°ƒèŠ‚å…¶æƒé‡ï¼›æ— éœ€é‡æ–°compileï¼›åˆ—è¡¨
                  optimizer=tf.keras.optimizers.experimental.AdamW(learning_rate=tf.keras.optimizers.schedules.CosineDecayRestarts(4.8e-3, 433), weight_decay=4.8e-3,
                                                                   use_ema=True, amsgrad=True, jit_compile=False), # Currently tf.py_function is not compatible with XLA. Calling tf.py_function inside tf.function(jit_compile=True) will raise an error
                  metrics=[['sparse_categorical_accuracy'], [], [], []],    # metricså¿…é¡»æ”¾åœ¨åˆ—è¡¨é‡Œï¼
                  run_eagerly=False) # ç”¨äºåœ¨modelä¸­ï¼ˆå¦‚è‡ªå®šä¹‰lossé‡Œï¼‰è·å–å…·ä½“çš„å€¼ï¼›ä½†eageræå…¶æ…¢ï¼
    return model

model = AAD(sources, channels, features, domains, AdjMat)

AdjMat_rec = np.zeros_like(AdjMat)  # ä¸è¦æ­£åˆ™åŒ–
for j in range(AdjMat_rec.shape[0]):
    AdjMat_rec[j,:,:] = percent(AdjMat[j,:,:])

model.fit(EEG_train,
          [LOC_train, np.repeat(np.expand_dims(AdjMat_rec,0), max(segments), axis=0), LOC_train, np.zeros((max(segments),1), dtype='int8')],    # å¿…é¡»ç»™y_trueï¼
          batch_size=batch_size, epochs=epochs, verbose=1, initial_epoch=0, shuffle=True,
          callbacks=[Weight(w0, w1, w2, w3)])